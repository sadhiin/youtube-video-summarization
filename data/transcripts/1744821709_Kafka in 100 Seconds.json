{
    "text": " Apache Kafka, a distributed event streaming platform that can scale massive pipelines of real-time data. It was created in 2011 at LinkedIn, written in Java and Scala, and named Kafka because it's a system optimized for writing. Let's imagine using it to build a dashboard like Google Analytics. When an event occurs, like a website visit, the Producer API creates a new record. These records are stored to disk in an ordered immutable log called a topic, which can persist forever or disappear when no longer needed. Topics are distributed and replicated in a cluster, which contains multiple servers called brokers. This makes Kafka fault-tolerant and able to scale to any workload. On the other side, multiple consumers can subscribe to this data. They can read the most recent message like a queue, or read the entire topic log, and listen to updates in real time. In addition, it provides a very powerful streams API that can transform and aggregate these topics before they ever reach the consumer. This all may sound similar to message brokers like RabbitMQ, but Kafka can handle more throughput and is ideal for streaming data applications. For example, it's used today by companies like Lyft to collect and process geolocation data, Spotify and Netflix for log processing, and Cloudflare for real analytics To get started download it and use a tool like Zookeeper or KRAFT to manage your cluster Now in one terminal start Zookeeper then in the other start the Kafka server With the environment up and running we can now create our first topic Remember, a topic is just a log of events kept in order. An event will have a key, value, and timestamp, and may also contain optional metadata and headers. Now, let's use this command to publish an event to the topic, where every line represents a different event. And now, these events are written to a topic, which is stored durably and partitioned in the cluster. Kafka guarantees that any consumer of a given topic will always read the events in the exact same order. Now with this command, we're able to consume the topic. By default, it will give us the latest event, although we can use the from beginning flag to read the entire log. It's also possible to provide an offset to read a subset of records. At this point, we've achieved basic event streaming, but the Kafka Streams API can take things to another level. It's most well supported with Java, and can do things like stateless transformation, like filtering out a subset of events, or stateful transformation, like an aggregation that combines multiple events into a single value over a certain window of time. And at that point, you're able to manage real-time streams of data at virtually any scale. This has been Apache Kafka in 100 seconds. Hit the like button and subscribe for more short videos like this. Thanks for watching, and I will see you in the next one.",
    "task": "transcribe",
    "language": "English",
    "duration": 154.2605,
    "segments": [
        {
            "id": 0,
            "seek": 0,
            "start": 0.24,
            "end": 6.28,
            "text": " Apache Kafka, a distributed event streaming platform that can scale massive pipelines of real-time data.",
            "tokens": [
                50377,
                46597,
                47064,
                11,
                257,
                12631,
                2280,
                11791,
                3663,
                300,
                393,
                4373,
                5994,
                40168,
                295,
                957,
                12,
                3766,
                1412,
                13,
                50679
            ],
            "temperature": 0,
            "avg_logprob": -0.09564248,
            "compression_ratio": 1.5386904,
            "no_speech_prob": 8.1962193e-13
        },
        {
            "id": 1,
            "seek": 0,
            "start": 6.42,
            "end": 13.36,
            "text": " It was created in 2011 at LinkedIn, written in Java and Scala, and named Kafka because it's a system optimized for writing.",
            "tokens": [
                50686,
                467,
                390,
                2942,
                294,
                10154,
                412,
                20657,
                11,
                3720,
                294,
                10745,
                293,
                2747,
                5159,
                11,
                293,
                4926,
                47064,
                570,
                309,
                311,
                257,
                1185,
                26941,
                337,
                3579,
                13,
                51033
            ],
            "temperature": 0,
            "avg_logprob": -0.09564248,
            "compression_ratio": 1.5386904,
            "no_speech_prob": 8.1962193e-13
        },
        {
            "id": 2,
            "seek": 0,
            "start": 13.48,
            "end": 16.72,
            "text": " Let's imagine using it to build a dashboard like Google Analytics.",
            "tokens": [
                51039,
                961,
                311,
                3811,
                1228,
                309,
                281,
                1322,
                257,
                18342,
                411,
                3329,
                25944,
                13,
                51201
            ],
            "temperature": 0,
            "avg_logprob": -0.09564248,
            "compression_ratio": 1.5386904,
            "no_speech_prob": 8.1962193e-13
        },
        {
            "id": 3,
            "seek": 0,
            "start": 17.12,
            "end": 21.98,
            "text": " When an event occurs, like a website visit, the Producer API creates a new record.",
            "tokens": [
                51221,
                1133,
                364,
                2280,
                11843,
                11,
                411,
                257,
                3144,
                3441,
                11,
                264,
                33034,
                9362,
                7829,
                257,
                777,
                2136,
                13,
                51464
            ],
            "temperature": 0,
            "avg_logprob": -0.09564248,
            "compression_ratio": 1.5386904,
            "no_speech_prob": 8.1962193e-13
        },
        {
            "id": 4,
            "seek": 0,
            "start": 22.14,
            "end": 29.6,
            "text": " These records are stored to disk in an ordered immutable log called a topic, which can persist forever or disappear when no longer needed.",
            "tokens": [
                51472,
                1981,
                7724,
                366,
                12187,
                281,
                12355,
                294,
                364,
                8866,
                3397,
                32148,
                3565,
                1219,
                257,
                4829,
                11,
                597,
                393,
                13233,
                5680,
                420,
                11596,
                562,
                572,
                2854,
                2978,
                13,
                51845
            ],
            "temperature": 0,
            "avg_logprob": -0.09564248,
            "compression_ratio": 1.5386904,
            "no_speech_prob": 8.1962193e-13
        },
        {
            "id": 5,
            "seek": 2960,
            "start": 29.6,
            "end": 34.96,
            "text": " Topics are distributed and replicated in a cluster, which contains multiple servers called brokers.",
            "tokens": [
                50365,
                8840,
                1167,
                366,
                12631,
                293,
                46365,
                294,
                257,
                13630,
                11,
                597,
                8306,
                3866,
                15909,
                1219,
                47549,
                13,
                50633
            ],
            "temperature": 0,
            "avg_logprob": -0.08020294,
            "compression_ratio": 1.6418918,
            "no_speech_prob": 6.6896076e-13
        },
        {
            "id": 6,
            "seek": 2960,
            "start": 35.26,
            "end": 38.94,
            "text": " This makes Kafka fault-tolerant and able to scale to any workload.",
            "tokens": [
                50648,
                639,
                1669,
                47064,
                7441,
                12,
                83,
                27035,
                394,
                293,
                1075,
                281,
                4373,
                281,
                604,
                20139,
                13,
                50832
            ],
            "temperature": 0,
            "avg_logprob": -0.08020294,
            "compression_ratio": 1.6418918,
            "no_speech_prob": 6.6896076e-13
        },
        {
            "id": 7,
            "seek": 2960,
            "start": 39.08,
            "end": 42.3,
            "text": " On the other side, multiple consumers can subscribe to this data.",
            "tokens": [
                50839,
                1282,
                264,
                661,
                1252,
                11,
                3866,
                11883,
                393,
                3022,
                281,
                341,
                1412,
                13,
                51000
            ],
            "temperature": 0,
            "avg_logprob": -0.08020294,
            "compression_ratio": 1.6418918,
            "no_speech_prob": 6.6896076e-13
        },
        {
            "id": 8,
            "seek": 2960,
            "start": 42.52,
            "end": 48.4,
            "text": " They can read the most recent message like a queue, or read the entire topic log, and listen to updates in real time.",
            "tokens": [
                51011,
                814,
                393,
                1401,
                264,
                881,
                5162,
                3636,
                411,
                257,
                18639,
                11,
                420,
                1401,
                264,
                2302,
                4829,
                3565,
                11,
                293,
                2140,
                281,
                9205,
                294,
                957,
                565,
                13,
                51305
            ],
            "temperature": 0,
            "avg_logprob": -0.08020294,
            "compression_ratio": 1.6418918,
            "no_speech_prob": 6.6896076e-13
        },
        {
            "id": 9,
            "seek": 2960,
            "start": 48.5,
            "end": 55.78,
            "text": " In addition, it provides a very powerful streams API that can transform and aggregate these topics before they ever reach the consumer.",
            "tokens": [
                51310,
                682,
                4500,
                11,
                309,
                6417,
                257,
                588,
                4005,
                15842,
                9362,
                300,
                393,
                4088,
                293,
                26118,
                613,
                8378,
                949,
                436,
                1562,
                2524,
                264,
                9711,
                13,
                51674
            ],
            "temperature": 0,
            "avg_logprob": -0.08020294,
            "compression_ratio": 1.6418918,
            "no_speech_prob": 6.6896076e-13
        },
        {
            "id": 10,
            "seek": 5578,
            "start": 55.78,
            "end": 61.02,
            "text": " This all may sound similar to message brokers like RabbitMQ, but Kafka can handle more throughput",
            "tokens": [
                50365,
                639,
                439,
                815,
                1626,
                2531,
                281,
                3636,
                47549,
                411,
                42092,
                44,
                48,
                11,
                457,
                47064,
                393,
                4813,
                544,
                44629,
                50627
            ],
            "temperature": 0,
            "avg_logprob": -0.12089579,
            "compression_ratio": 1.5833334,
            "no_speech_prob": 5.168719e-13
        },
        {
            "id": 11,
            "seek": 5578,
            "start": 61.02,
            "end": 66.36,
            "text": " and is ideal for streaming data applications. For example, it's used today by companies like",
            "tokens": [
                50627,
                293,
                307,
                7157,
                337,
                11791,
                1412,
                5821,
                13,
                1171,
                1365,
                11,
                309,
                311,
                1143,
                965,
                538,
                3431,
                411,
                50894
            ],
            "temperature": 0,
            "avg_logprob": -0.12089579,
            "compression_ratio": 1.5833334,
            "no_speech_prob": 5.168719e-13
        },
        {
            "id": 12,
            "seek": 5578,
            "start": 66.36,
            "end": 72.02,
            "text": " Lyft to collect and process geolocation data, Spotify and Netflix for log processing, and",
            "tokens": [
                50894,
                12687,
                844,
                281,
                2500,
                293,
                1399,
                1519,
                401,
                27943,
                1412,
                11,
                29036,
                293,
                12778,
                337,
                3565,
                9007,
                11,
                293,
                51177
            ],
            "temperature": 0,
            "avg_logprob": -0.12089579,
            "compression_ratio": 1.5833334,
            "no_speech_prob": 5.168719e-13
        },
        {
            "id": 13,
            "seek": 72,
            "start": 72.02,
            "end": 86.080505,
            "text": " Cloudflare for real analytics To get started download it and use a tool like Zookeeper or KRAFT to manage your cluster Now in one terminal start Zookeeper then in the other start the Kafka server With the environment up and running we can now create our first topic",
            "tokens": [
                51177,
                8061,
                3423,
                543,
                337,
                957,
                12,
                3766,
                15370,
                13,
                1407,
                483,
                1409,
                11,
                5484,
                309,
                293,
                764,
                257,
                2290,
                411,
                34589,
                23083,
                420,
                51440,
                51440,
                591,
                3750,
                25469,
                281,
                3067,
                428,
                13630,
                13,
                823,
                11,
                294,
                472,
                14709,
                11,
                722,
                34589,
                23083,
                11,
                550,
                294,
                264,
                661,
                11,
                50839,
                50849,
                722,
                264,
                47064,
                7154,
                13,
                2022,
                264,
                2823,
                493,
                293,
                2614,
                11,
                321,
                393,
                586,
                1884,
                527,
                700,
                4829,
                13,
                51062
            ],
            "temperature": 0,
            "avg_logprob": -0.090907685,
            "compression_ratio": 1.6498517,
            "no_speech_prob": 8.356858e-13
        },
        {
            "id": 14,
            "seek": 72,
            "start": 86.4205,
            "end": 91.16051,
            "text": " Remember, a topic is just a log of events kept in order. An event will have a key,",
            "tokens": [
                51079,
                5459,
                11,
                257,
                4829,
                307,
                445,
                257,
                3565,
                295,
                3931,
                4305,
                294,
                1668,
                13,
                1107,
                2280,
                486,
                362,
                257,
                2141,
                11,
                51316
            ],
            "temperature": 0,
            "avg_logprob": -0.090907685,
            "compression_ratio": 1.6498517,
            "no_speech_prob": 8.356858e-13
        },
        {
            "id": 15,
            "seek": 72,
            "start": 91.3805,
            "end": 96.6205,
            "text": " value, and timestamp, and may also contain optional metadata and headers. Now, let's use this command",
            "tokens": [
                51327,
                2158,
                11,
                293,
                49108,
                1215,
                11,
                293,
                815,
                611,
                5304,
                17312,
                26603,
                293,
                45101,
                13,
                823,
                11,
                718,
                311,
                764,
                341,
                5622,
                51589
            ],
            "temperature": 0,
            "avg_logprob": -0.090907685,
            "compression_ratio": 1.6498517,
            "no_speech_prob": 8.356858e-13
        },
        {
            "id": 16,
            "seek": 72,
            "start": 96.6205,
            "end": 101.3405,
            "text": " to publish an event to the topic, where every line represents a different event. And now,",
            "tokens": [
                51589,
                281,
                11374,
                364,
                2280,
                281,
                264,
                4829,
                11,
                689,
                633,
                1622,
                8855,
                257,
                819,
                2280,
                13,
                400,
                586,
                11,
                51825
            ],
            "temperature": 0,
            "avg_logprob": -0.090907685,
            "compression_ratio": 1.6498517,
            "no_speech_prob": 8.356858e-13
        },
        {
            "id": 17,
            "seek": 2992,
            "start": 101.3405,
            "end": 105.440506,
            "text": " these events are written to a topic, which is stored durably and partitioned in the cluster.",
            "tokens": [
                50365,
                613,
                3931,
                366,
                3720,
                281,
                257,
                4829,
                11,
                597,
                307,
                12187,
                4861,
                1188,
                293,
                24808,
                292,
                294,
                264,
                13630,
                13,
                50570
            ],
            "temperature": 0,
            "avg_logprob": -0.045637954,
            "compression_ratio": 1.6886227,
            "no_speech_prob": 7.289035e-13
        },
        {
            "id": 18,
            "seek": 2992,
            "start": 105.7005,
            "end": 110.2805,
            "text": " Kafka guarantees that any consumer of a given topic will always read the events in the exact",
            "tokens": [
                50583,
                47064,
                32567,
                300,
                604,
                9711,
                295,
                257,
                2212,
                4829,
                486,
                1009,
                1401,
                264,
                3931,
                294,
                264,
                1900,
                50812
            ],
            "temperature": 0,
            "avg_logprob": -0.045637954,
            "compression_ratio": 1.6886227,
            "no_speech_prob": 7.289035e-13
        },
        {
            "id": 19,
            "seek": 2992,
            "start": 110.2805,
            "end": 114.860504,
            "text": " same order. Now with this command, we're able to consume the topic. By default, it will give us",
            "tokens": [
                50812,
                912,
                1668,
                13,
                823,
                365,
                341,
                5622,
                11,
                321,
                434,
                1075,
                281,
                14732,
                264,
                4829,
                13,
                3146,
                7576,
                11,
                309,
                486,
                976,
                505,
                51041
            ],
            "temperature": 0,
            "avg_logprob": -0.045637954,
            "compression_ratio": 1.6886227,
            "no_speech_prob": 7.289035e-13
        },
        {
            "id": 20,
            "seek": 2992,
            "start": 114.860504,
            "end": 119.540504,
            "text": " the latest event, although we can use the from beginning flag to read the entire log. It's also",
            "tokens": [
                51041,
                264,
                6792,
                2280,
                11,
                4878,
                321,
                393,
                764,
                264,
                490,
                2863,
                7166,
                281,
                1401,
                264,
                2302,
                3565,
                13,
                467,
                311,
                611,
                51275
            ],
            "temperature": 0,
            "avg_logprob": -0.045637954,
            "compression_ratio": 1.6886227,
            "no_speech_prob": 7.289035e-13
        },
        {
            "id": 21,
            "seek": 2992,
            "start": 119.540504,
            "end": 124.6005,
            "text": " possible to provide an offset to read a subset of records. At this point, we've achieved basic",
            "tokens": [
                51275,
                1944,
                281,
                2893,
                364,
                18687,
                281,
                1401,
                257,
                25993,
                295,
                7724,
                13,
                1711,
                341,
                935,
                11,
                321,
                600,
                11042,
                3875,
                51528
            ],
            "temperature": 0,
            "avg_logprob": -0.045637954,
            "compression_ratio": 1.6886227,
            "no_speech_prob": 7.289035e-13
        },
        {
            "id": 22,
            "seek": 2992,
            "start": 124.6005,
            "end": 129.1205,
            "text": " event streaming, but the Kafka Streams API can take things to another level. It's most well",
            "tokens": [
                51528,
                2280,
                11791,
                11,
                457,
                264,
                47064,
                24904,
                82,
                9362,
                393,
                747,
                721,
                281,
                1071,
                1496,
                13,
                467,
                311,
                881,
                731,
                51754
            ],
            "temperature": 0,
            "avg_logprob": -0.045637954,
            "compression_ratio": 1.6886227,
            "no_speech_prob": 7.289035e-13
        },
        {
            "id": 23,
            "seek": 5770,
            "start": 129.1205,
            "end": 133.4405,
            "text": " supported with Java, and can do things like stateless transformation, like filtering out a",
            "tokens": [
                50365,
                8104,
                365,
                10745,
                11,
                293,
                393,
                360,
                721,
                411,
                2219,
                4272,
                9887,
                11,
                411,
                30822,
                484,
                257,
                50581
            ],
            "temperature": 0,
            "avg_logprob": -0.04109736,
            "compression_ratio": 1.6390729,
            "no_speech_prob": 4.633808e-13
        },
        {
            "id": 24,
            "seek": 5770,
            "start": 133.4405,
            "end": 139.16049,
            "text": " subset of events, or stateful transformation, like an aggregation that combines multiple events into",
            "tokens": [
                50581,
                25993,
                295,
                3931,
                11,
                420,
                1785,
                906,
                9887,
                11,
                411,
                364,
                16743,
                399,
                300,
                29520,
                3866,
                3931,
                666,
                50867
            ],
            "temperature": 0,
            "avg_logprob": -0.04109736,
            "compression_ratio": 1.6390729,
            "no_speech_prob": 4.633808e-13
        },
        {
            "id": 25,
            "seek": 5770,
            "start": 139.16049,
            "end": 143.9805,
            "text": " a single value over a certain window of time. And at that point, you're able to manage real-time",
            "tokens": [
                50867,
                257,
                2167,
                2158,
                670,
                257,
                1629,
                4910,
                295,
                565,
                13,
                400,
                412,
                300,
                935,
                11,
                291,
                434,
                1075,
                281,
                3067,
                957,
                12,
                3766,
                51108
            ],
            "temperature": 0,
            "avg_logprob": -0.04109736,
            "compression_ratio": 1.6390729,
            "no_speech_prob": 4.633808e-13
        },
        {
            "id": 26,
            "seek": 5770,
            "start": 143.9805,
            "end": 149.24051,
            "text": " streams of data at virtually any scale. This has been Apache Kafka in 100 seconds. Hit the like",
            "tokens": [
                51108,
                15842,
                295,
                1412,
                412,
                14103,
                604,
                4373,
                13,
                639,
                575,
                668,
                46597,
                47064,
                294,
                2319,
                3949,
                13,
                9217,
                264,
                411,
                51371
            ],
            "temperature": 0,
            "avg_logprob": -0.04109736,
            "compression_ratio": 1.6390729,
            "no_speech_prob": 4.633808e-13
        },
        {
            "id": 27,
            "seek": 5770,
            "start": 149.24051,
            "end": 154.0805,
            "text": " button and subscribe for more short videos like this. Thanks for watching, and I will see you in the next one.",
            "tokens": [
                51371,
                2960,
                293,
                3022,
                337,
                544,
                2099,
                2145,
                411,
                341,
                13,
                2561,
                337,
                1976,
                11,
                293,
                286,
                486,
                536,
                291,
                294,
                264,
                958,
                472,
                13,
                51613
            ],
            "temperature": 0,
            "avg_logprob": -0.04109736,
            "compression_ratio": 1.6390729,
            "no_speech_prob": 4.633808e-13
        }
    ],
    "x_groq": {
        "id": "req_01jrzqmydsftx85r44fqrrjjf2"
    }
}